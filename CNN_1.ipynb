{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN实现手写识别 （18/06/06）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# 导入mnist的数据集\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# read_data_sets(train_dir,fake_data=False,one_hot=True,dtype=dtypes.float32,\n",
    "                   #reshape=True, validation_size=5000,seed=None)\n",
    "# 过程就是下载数据，解压数据，返回datasets，包括train，validation,test三个部分\n",
    "# one_hot = True 是考虑多类情况。非onehot，标签是0 1 2 3 这样的\n",
    "# one_hot 就是一个长度为 n 的数组，只有一个元素是1，其他元素是0\n",
    "# mnist的标签就是这样的，所以one_hot = True\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.12\n",
      "step 100, training accuracy 0.38\n",
      "step 200, training accuracy 0.48\n",
      "step 300, training accuracy 0.78\n",
      "step 400, training accuracy 0.78\n",
      "step 500, training accuracy 0.78\n",
      "step 600, training accuracy 0.82\n",
      "step 700, training accuracy 0.88\n",
      "step 800, training accuracy 0.92\n",
      "step 900, training accuracy 0.86\n",
      "step 999, training accuracy 0.86\n"
     ]
    }
   ],
   "source": [
    "# tf.placeholder(dtype, shape=None, name=None)\n",
    "# 此函数可以理解为形参，用于定义过程，在执行的时候再赋具体的值\n",
    "# shape：数据形状。比如[2,3], [None, 10]表示列是10，行不定\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])  \n",
    "y_ = tf.placeholder(tf.float32, [None, 10])  \n",
    "\n",
    "# TensorFlow程序的流程是先创建一个图，然后在session中启动它\n",
    "sess = tf.InteractiveSession()  \n",
    "\n",
    "# 用于初始化 W\n",
    "def weight_variable(shape):  \n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)  \n",
    "    return tf.Variable(initial)  \n",
    "\n",
    "# 用于初始化 b\n",
    "def bias_variable(shape):  \n",
    "    initial = tf.constant(0.1, shape=shape)  \n",
    "    return tf.Variable(initial)  \n",
    "\n",
    "# 用于构建卷积层  \n",
    "# tf.nn.conv2d(input, filter, strides, padding）\n",
    "# input：[batch, in_height, in_width, in_channels]\n",
    "# filter: [filter_height, filter_width, in_channels, out_channels]\n",
    "def conv2d(x, W):  \n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') \n",
    "\n",
    "\n",
    "# 用于构建池化层  \n",
    "# tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "# value：需要池化的输入，一般池化层接在卷积层后面，所以输入通常是feature map\n",
    "# ksize: 池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]\n",
    "       # 因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "# strides：和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "# padding：和卷积类似，可以取'VALID' 或者'SAME'\n",
    "def max_pool_2x2(x):  \n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],  \n",
    "                          strides=[1, 2, 2, 1], padding='SAME')  \n",
    "# 池化窗口是2 X 2的\n",
    "# 整个网络两个卷积层，一个全连接层，一个dropout层，一个softmax层\n",
    "\n",
    "# 第一层卷积  \n",
    "W_conv1 = weight_variable([5, 5, 1, 32])  \n",
    "b_conv1 = bias_variable([32])  \n",
    "\n",
    "# tf.reshape(tensor, shape, name=None) \n",
    "# 函数的作用是将tensor变换为参数shape的形式\n",
    "# -1 表示，先按28 x 28来 ，然后有多少层，系统安排\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1])  \n",
    "# 第一个卷积层\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)  \n",
    "# conv2d 根据前面的解释，所以这里的filter是5X5的，通道是32个\n",
    "# 第一个池化层\n",
    "h_pool1 = max_pool_2x2(h_conv1)  \n",
    "\n",
    "\n",
    "\n",
    "# 第二层卷积  \n",
    "W_conv2 = weight_variable([5, 5, 32, 64])  \n",
    "b_conv2 = bias_variable([64])  \n",
    "# 第二个卷积层\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2) \n",
    "# filter 还是5X5，通道是64了\n",
    "\n",
    "# 第二个池化层\n",
    "h_pool2 = max_pool_2x2(h_conv2) \n",
    "\n",
    "# 全连接层  \n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])  \n",
    "b_fc1 = bias_variable([1024])  \n",
    "\n",
    "# reshape成向量\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7 * 7 * 64])  \n",
    "\n",
    "# 第一个全连接层\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  \n",
    "\n",
    "# dropput层  \n",
    "keep_prob = tf.placeholder(tf.float32)  \n",
    "# tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None, name=None)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "# keep_prob 是保留概率\n",
    "\n",
    "\n",
    "# 输出层  \n",
    "W_fc2 = weight_variable([1024, 10])  \n",
    "b_fc2 = bias_variable([10])  \n",
    "\n",
    "\n",
    "# y_predict = tf.matmul(h_fc1_drop, W_fc2) + b_fc2  \n",
    "# 或变成softmax层 \n",
    "y_predict=tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "\n",
    "#训练和评估模型  \n",
    "# cross_entropy\n",
    "\n",
    "# tf.nn.softmax_cross_entropy_with_logits(logits, labels, name=None)\n",
    "# logits 就是神经网路最后一层的输出 \n",
    "# labels就是实际的标签，用这两者做比较\n",
    "# 这个函数返回的是一个向量，要求loss就还要对向量求均值，所以用reduce_mean\n",
    "cross_entropy = tf.reduce_mean(  \n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_predict)) \n",
    "# 用的AdamOptimizer\n",
    "# 也可以用GradientDescentOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)  \n",
    "# train_step = tf.train.GradientDescentOptimizer(1e-4).minimize(cross_entropy)  \n",
    "\n",
    "# 算正确率\n",
    "correct_prediction = tf.equal(tf.argmax(y_predict, 1), tf.argmax(y_, 1))  \n",
    "\n",
    "# 运行上面这个函数，会生成一组向量，如：[True, False, True, True]。\n",
    "# 把它映射成浮点数，然后，计算它们的均值\n",
    "# cast(x, dtype, name=None) ,将x的数据格式转化成dtype\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  \n",
    "\n",
    "#开始训练\n",
    "sess.run(tf.global_variables_initializer())  \n",
    "for i in range(1000):  \n",
    "    # 50一组 把mnist上的东西赋给了batch\n",
    "    # 之后x = batch[0],y_ = batch[1]\n",
    "    batch = mnist.train.next_batch(50) \n",
    "    \n",
    "    if i % 100 == 0:  \n",
    "        train_accuracy = accuracy.eval(feed_dict={  \n",
    "            x: batch[0], y_: batch[1], keep_prob: 1.0})  \n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))  \n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})  \n",
    "\n",
    "print(\"step %d, training accuracy %g\" % (i, train_accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
